{{Refimprove|date=January 2009}}
 
'''Computational neuroscience''' is the study of [[brain function]] in terms of the [[information processing]] properties of the structures that make up the [[nervous system]].<ref>What is computational neuroscience? Patricia S. Churchland, Christof Koch, Terrence J. Sejnowski. in Computational Neuroscience pp.46-55. Edited by Eric L. Schwartz. 1993. MIT Press [http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&tid=7195]</ref>  It is an interdisciplinary science that links the diverse fields of [[neuroscience]], [[cognitive science]] and [[cognitive psychology|psychology]] with [[electrical engineering]], [[computer science]], [[mathematics]] and [[physics]].  

Computational neuroscience is somewhat distinct from psychological [[connectionism]] and theories of learning from disciplines such as [[machine learning]], [[neural networks]] and [[computational learning theory]] in that it emphasizes descriptions of functional and biologically realistic neurons (and neural systems) and their physiology and dynamics.  These models capture the essential features of the biological system at multiple spatial-temporal scales, from membrane currents, protein and chemical coupling to [[neural oscillation|network oscillations]], columnar and topographic architecture and learning and memory.  These computational models are used to frame hypotheses that can be directly tested by current or future biological and/or psychological experiments.

==History==
The term "computational neuroscience" was introduced by [[Eric L. Schwartz]], who organized a conference, held in 1985 in Carmel, California at the request of the Systems Development Foundation, to provide a summary of the current status of a field which until that point was referred to by a variety of names, such as neural modeling, brain theory and neural networks. The proceedings of this definitional meeting were later published as the book "Computational Neuroscience" (1990).<ref>{{cite book |author=Schwartz, Eric |title=Computational neuroscience |publisher=MIT Press |location=Cambridge, Mass |year=1990 |pages= |isbn=0-262-19291-8 }}</ref> 

The early historical roots of the field can be traced to the work of people such as [[Louis Lapicque]], [[Alan Hodgkin|Hodgkin]] & [[Andrew Huxley|Huxley]], [[David H. Hubel|Hubel]] & [[Torsten Wiesel|Wiesel]], and [[David Marr (psychologist)|David Marr]], to name but a few. Lapicque introduced the [[integrate and fire]] model of the neuron in a seminal article published in 1907;<ref>{{cite journal |author=Lapicque L |title= Recherches quantitatives sur l'excitation électrique des nerfs traitée comme une polarisation |journal=J. Physiol. Pathol. Gen. |volume=9 |pages=620–635 |year=1907}}</ref> this model is still one of the most popular models in computational neuroscience for both cellular and [[neural networks]] studies, as well as in [[mathematical neuroscience]] because of its simplicity (see the recent review article <ref>{{cite journal |author=Brunel N, Van Rossum MC |title= Lapicque's 1907 paper: from frogs to integrate-and-fire |journal=Biol. Cybern. |volume=97 |pages=337–339 |year=2007 |pmid=17968583 |doi=10.1007/s00422-007-0190-0 |issue=5-6}}</ref> published recently for the centenary of the original Lapicque's 1907 paper - this review also contains an English translation of the original paper).  About 40 years later, Hodgkin & Huxley developed the voltage clamp and created the first biophysical model of the [[action potential]]. Hubel & Wiesel discovered that neurons in the [[primary visual cortex]], the first cortical area to process information coming from the [[retina]], have oriented receptive fields and are organized in columns.<ref>{{cite journal |author=Hubel DH, Wiesel TN |title=Receptive fields, binocular interaction and functional architecture in the cat's visual cortex |journal=J. Physiol. (Lond.) |volume=160 |pages=106–54 |year=1962 |pmid=14449617 |pmc=1359523 |doi= |url=http://www.jphysiol.org/cgi/pmidlookup?view=long&pmid=14449617}}</ref> David Marr's work focused on the interactions between neurons, suggesting computational approaches to the study of how functional groups of neurons within the [[hippocampus]] and [[neocortex]] interact, store, process, and transmit information.  Computational modeling of biophysically realistic neurons and dendrites began with the work of [[Wilfrid Rall]], with the first multicompartmental model using [[cable theory]].

==Major topics==
Research in computational neuroscience can be roughly categorized into several lines of inquiries.  Most computational neuroscientists collaborate closely with experimentalists in analyzing novel data and synthesizing new models of biological phenomena.

===Single-neuron modeling===
{{main|Biological neuron models}}
Even single neurons have complex biophysical characteristics.  Hodgkin and Huxley's [[Hodgkin-Huxley model|original model]] only employed two voltage-sensitive currents, the fast-acting sodium and the inward-rectifying potassium.  Though successful in predicting the timing and qualitative features of the action potential, it nevertheless failed to predict a number of important features such as adaptation and shunting. Scientists now believe that there are a wide variety of voltage-sensitive currents, and the implications of the differing dynamics, modulations and sensitivity of these currents is an important topic of computational neuroscience.<ref>{{cite book |author=Wu, Samuel Miao-sin; Johnston, Daniel |title=Foundations of cellular neurophysiology |publisher=MIT Press |location=Cambridge, Mass |year=1995 |isbn=0-262-10053-3 }}</ref>

The computational functions of complex [[dendrites]] are also under intense investigation.  There is a large body of literature regarding how different currents interact with geometric properties of neurons.<ref>{{cite book |author=Koch, Christof |title=Biophysics of computation: information processing in single neurons |publisher=Oxford University Press |location=Oxford [Oxfordshire] |year=1999 |isbn=0-19-510491-9 }}</ref>

Some models are also tracking biochemical pathways at very small scales such as spines or synaptic clefts.

There are many software packages, such as  [[GENESIS (software)|GENESIS]] and [[Neuron (software)|NEURON]], that allow rapid and systematic ''in silico'' modeling of realistic neurons. [[Blue Brain]], a project founded by [[Henry Markram]] from the [[École Polytechnique Fédérale de Lausanne]], aims to construct a biophysically detailed simulation of a [[cortical column]] on the [[Blue Gene]] [[supercomputer]].

===Development, axonal patterning and guidance===
How do [[axons]] and [[dendrites]] form during development?  How do axons know where to target and how to reach these targets?  How do neurons migrate to the proper position in the central and peripheral systems?  How do synapses form?  We know from molecular biology that distinct parts of the nervous system release distinct chemical cues, from [[growth factors]] to [[hormones]] that modulate and influence the growth and development of functional connections between neurons.  

Theoretical investigations into the formation and patterning of synaptic connection and morphology are still nascent.  One hypothesis that has recently garnered some attention is the ''minimal wiring hypothesis'', which postulates that the formation of axons and dendrites effectively minimizes resource allocation while maintaining maximal information storage.<ref>{{cite journal |author=Chklovskii DB, Mel BW, Svoboda K |title=Cortical rewiring and information storage |journal=Nature |volume=431 |issue=7010 |pages=782–8 |year=2004 |month= October|pmid=15483599 |doi=10.1038/nature03012 |bibcode = 2004Natur.431..782C }}<br/>Review article</ref>

===Sensory processing===
Early models of sensory processing understood within a theoretical framework is credited to [[Horace Barlow]].  Somewhat similar to the minimal wiring hypothesis described in the preceding section, Barlow understood the processing of the early sensory systems to be a form of [[efficient coding hypothesis|efficient coding]], where the neurons encoded information which minimized the number of spikes.  Experimental and computational work have since supported this hypothesis in one form or another.  

Current research in sensory processing is divided among biophysical modelling of different subsystems and more theoretical modelling of perception.  Current models of perception have suggested that the brain performs some form of [[Bayesian inference]] and integration of different sensory information in generating our perception of the physical world.

===Memory and synaptic plasticity===
{{main|Synaptic plasticity}}
Earlier models of memory are primarily based on the postulates of [[Hebbian learning]].  Biologically relevant models such as [[Hopfield net]] have been developed to address the properties of associative, rather than content-addressable style of memory that occur in biological systems.  These attempts are primarily focusing on the formation of medium-term and long-term memory, localizing in the hippocampus.  Models of working memory, relying on theories of network oscillations and persistent activity, have been built to capture some features of the prefrontal cortex in context-related memory.<ref>{{cite journal |author=Durstewitz D, Seamans JK, Sejnowski TJ |title=Neurocomputational models of working memory |journal=Nat Neurosci. |volume=3 |issue=Suppl |pages=1184–91 |year=2000 |pmid=11127836 |doi=10.1038/81460 |url=}}</ref>

One of the major problems in neurophysiological memory is how it is maintained and changed through multiple time scales.  Unstable [[synapses]] are easy to train but also prone to stochastic disruption.  Stable [[synapses]] forget less easily, but they are also harder to consolidate.  One recent computational hypothesis involves cascades of plasticity<ref>{{cite journal |author=Fusi S, Drew PJ, Abbott LF |title=Cascade models of synaptically stored memories |journal=Neuron |volume=45 |issue=4 |pages=599–611 |year=2005 |pmid=15721245 |doi=10.1016/j.neuron.2005.02.001 }}</ref> that allow synapses to function at multiple time scales.  Stereochemically detailed models of the [[acetylcholine receptor]]-based synapse with [[Monte Carlo method]], working at the time scale of microseconds, have been built.<ref>{{cite journal |author=Coggan JS, Bartol TM, Esquenazi E, ''et al.'' |title=Evidence for ectopic neurotransmission at a neuronal synapse |journal=Science |volume=309 |issue=5733 |pages=446–51 |year=2005 |pmid=16020730 |pmc=2915764 |doi=10.1126/science.1108239 |bibcode = 2005Sci...309..446C }}</ref>  It is likely that computational tools will contribute greatly to our understanding of how synapses function and change in relation to external stimulus in the coming decades.

===Behaviors of networks===
Biological neurons are connected to each other in a complex, recurrent fashion.  These connections are, unlike most [[artificial neural networks]], sparse and most likely, specific.  It is not known how information is transmitted through such sparsely connected networks.  It is also unknown what the computational functions, if any, of these specific connectivity patterns are.

The interactions of neurons in a small network can be often reduced to simple models such as the [[Ising model]].    The [[statistical mechanics]] of such simple systems are well-characterized theoretically.  There has been some recent evidence that suggests that dynamics of arbitrary neuronal networks can be reduced to pairwise interactions.(Schneidman et al., 2006; Shlens et al., 2006.)<ref>{{cite journal |author=Schneidman E, Berry MJ, Segev R, Bialek W |title=Weak pairwise correlations imply strongly correlated network states in a neural population |journal=Nature |volume=440 |issue=7087 |pages=1007–12 |year=2006 |pmid=16625187 |pmc=1785327 |doi=10.1038/nature04701 |bibcode=2006Natur.440.1007S|arxiv = q-bio/0512013 }}</ref>  It's unknown, however, whether such descriptive dynamics impart any important computational function.  With the emergence of [[two-photon microscopy]] and [[calcium imaging]], we now have powerful experimental methods with which to test the new theories regarding neuronal networks. 

In some cases the complex interactions between ''inhibitory'' and ''excitatory'' neurons can be simplified using [[mean field theory]] that gives rise to [[Wilson-Cowan model|population model]] of neural networks. While many neuro-theorists prefer such models with reduced complexity, others argue that uncovering structure function relations depends on including as much neuronal and network structure as possible.  Models of this type are typically built in large simulations platforms like  [[GENESIS (software)|GENESIS]] or [[Neuron (software)|Neuron]].  There have been some attempts to provide unified methods that bridge and integrate these levels of complexity.<ref>{{cite book |author=Anderson, Charles H.; Eliasmith, Chris |title=Neural Engineering: Computation, Representation, and Dynamics in Neurobiological Systems (Computational Neuroscience) |publisher=The MIT Press |location=Cambridge, Mass |year=2004 |pages= |isbn=0-262-55060-1 }}</ref>

===Cognition, discrimination and learning===
Computational modeling of higher cognitive functions has only recently begun.  Experimental data comes primarily from [[single-unit recording]] in [[primates]].  The [[frontal lobe]] and [[parietal lobe]] function as integrators of information from multiple sensory modalities.  There are some tentative ideas regarding how simple mutually inhibitory functional circuits in these areas may carry out biologically relevant computation.<ref>{{cite journal |author=Machens CK, Romo R, Brody CD |title=Flexible control of mutual inhibition: a neural model of two-interval discrimination |journal=Science |volume=307 |issue=5712 |pages=1121–4 |year=2005 |pmid=15718474 |doi=10.1126/science.1104171 |bibcode = 2005Sci...307.1121M }}</ref>

The [[brain]] seems to be able to discriminate and adapt particularly well in certain contexts.  For instance, human beings seem to have an enormous capacity for memorizing and recognizing faces.  One of the key goals of computational neuroscience is to dissect how biological systems carry out these complex computations efficiently and potentially replicate these processes in building intelligent machines.  

The brain's large-scale organizational principles are illuminated by many fields, including biology, psychology, and clinical practice.  [[Integrative neuroscience]] attempts to consolidate these observations through unified descriptive models and databases of behavioral measures and recordings.  These are the basis for some quantitative modeling of large-scale brain activity.<ref>{{cite journal |author=Robinson PA, Rennie CJ, Rowe DL, O'Connor SC, Gordon E | title=Multiscale brain modelling | journal=Philosophical Transactions of the Royal Society B | volume=360 | issue=1457|pages=1043–1050|year=2005|doi=10.1098/rstb.2005.1638 |pmid=16087447 |pmc=1854922 }}</ref>

===Consciousness===
One of the ultimate goals of psychology/neuroscience is to be able to explain the everyday experience of conscious life.    [[Francis Crick]] and [[Christof Koch]] made some attempts in formulating a consistent framework for future work in [[neural correlates of consciousness]] (NCC), though much of the work in this field remains speculative.<ref>{{cite journal |author=Crick F, Koch C |title=A framework for consciousness |journal=Nat Neurosci. |volume=6 |issue=2 |pages=119–26 |year=2003 |pmid=12555104 |doi=10.1038/nn0203-119 }}</ref>

==See also==
* [[Connectionism]]
* [[Neural network]]
* [[Biological neuron models]]
* [[Neural coding]]
* [[Brain-computer interface]]
* [[Neural engineering]]
* [[Neuroinformatics]]

==References==
===Notes===
{{Reflist}}

===General references===
* {{cite journal |author=Chklovskii DB |title=Synaptic connectivity and neuronal morphology: two sides of the same coin |journal=Neuron |volume=43 |issue=5 |pages=609–17 |year=2004 |pmid=15339643 |doi=10.1016/j.neuron.2004.08.012 }}
* {{cite book |author=Sejnowski, Terrence J.; Churchland, Patricia Smith |title=The computational brain |publisher=[[MIT Press]] |location=Cambridge, Mass |year=1992 |isbn=0-262-03188-4 }}
* {{cite book |author=Abbott, L. F.; Dayan, Peter |title=Theoretical neuroscience: computational and mathematical modeling of neural systems |publisher=MIT Press |location=Cambridge, Mass |year=2001 |isbn=0-262-04199-5 }}
* {{cite book |author=Eliasmith, Chris; Anderson, Charles H. |title=Neural engineering: Representation, computation, and dynamcs in neurobiological systems |publisher=[[MIT Press]] |location=Cambridge, Mass |year=2003 |isbn=0-262-05071-4 }}
* {{cite journal |author=Hodgkin AL, Huxley AF |title=A quantitative description of membrane current and its application to conduction and excitation in nerve |journal=J Physiol. (Lond.) |volume=117 |issue=4 |pages=500–44 |date=28 August 1952|pmid=12991237 |pmc=1392413 |url=http://www.jphysiol.org/cgi/pmidlookup?view=long&pmid=12991237 }}
* {{cite book |author=William Bialek; Rieke, Fred; David Warland; Rob de Ruyter van Steveninck |title=Spikes: exploring the neural code |publisher=MIT |location=Cambridge, Mass |year=1999 |isbn=0-262-68108-0 }}
* {{cite book |author=Schutter, Erik de |title=Computational neuroscience: realistic modeling for experimentalists |publisher=CRC |location=Boca Raton |year=2001 |isbn=0-8493-2068-2 }}
* {{cite book |author=Sejnowski, Terrence J.; Hemmen, J. L. van |title=23 problems in systems neuroscience |publisher=Oxford University Press |location=Oxford [Oxfordshire] |year=2006 |isbn=0-19-514822-3 }}
* {{cite book |author=Michael A. Arbib, Shun-ichi Amari, Prudence H. Arbib| title=The Handbook of Brain Theory and Neural Networks|publisher=The MIT Press|location=Cambridge, Massachusetts |year=2002 |isbn=0-262-01197-2}}


==External links==
===Journals===
* [http://www.informaworld.com/network Network: Computation in Neural Systems]
* [http://www.springerlink.com/openurl.asp?genre=journal&issn=0340-1200 Biological Cybernetics]
* [http://www.springer.com/10827 Journal of Computational Neuroscience]
* [http://www.mitpressjournals.org/loi/neco Neural Computation]
* [http://www.sciencedirect.com/science/journal/08936080 Neural Networks]
* [http://www.elsevier.com/locate/neucom Neurocomputing]
* [http://www.springerlink.com/content/1871-4099/ Cognitive Neurodynamics]
* [http://frontiersin.org/neuroscience/computationalneuroscience/ Frontiers in Computational Neuroscience]
* [http://www.ploscompbiol.org/home.action PLoS Computational Biology]
* [http://www.frontiersin.org/Journal/specialty.aspx?s=752&name=neuroinformatics&x=y Frontiers in Neuroinformatics]

===Software===
* [[Emergent (software)|Emergent]], neural simulation software.
* [http://genesis-sim.org/ Genesis], a general neural simulation system.
* [http://senselab.med.yale.edu/modeldb ModelDB], a large open-access database of program codes of published computational neuroscience models.
* [http://www.nest-initiative.org NEST], a simulation tool for large neuronal systems.
* [http://www.neuroconstruct.org Neuroconstruct], software for developing biologically realistic 3D neural networks.
* [http://www.neuron.yale.edu/ NEURON], a neuron simulator also useful to simulate neural networks.
* [http://snnap.uth.tmc.edu/ SNNAP], a single neuron and neural network simulator tool.
* [http://remoto.leb.usp.br/remoto/index.html ReMoto], a web-based simulator of the spinal cord and innervated muscles of the human leg.

===Conferences===
* [http://www.cosyne.org Computational and Systems Neuroscience (COSYNE)]– a computational neuroscience meeting with a systems neuroscience focus.
* [http://www.cnsorg.org Annual Computational Neuroscience Meeting (CNS)]– a yearly computational neuroscience meeting.
* [http://www.nips.cc Neural Information Processing Systems (NIPS)]– a leading annual conference covering other machine learning topics as well.
* [http://www.ccnconference.org Computational Cognitive Neuroscience Conference (CCNC)]– a yearly conference.
* [http://www.iccn2007.org/ International Conference on Cognitive Neurodynamics (ICCN)]– a yearly conference.
* [http://www.icms.org.uk/workshops/mathneuro UK Mathematical Neurosciences Meeting]– a new yearly conference, focused on mathematical aspects.
* [http://www.neurocomp.fr/index.php?page=welcome The NeuroComp Conference]– a yearly computational neuroscience conference (France).
* [http://www.nncn.de/Aktuelles-en/bernsteinsymposium/Symposium/view?set_language=en Bernstein Conference on Computational Neuroscience (BCCN)]– a yearly conference in Germany, organized by the [http://www.nncn.de/willkommen-en/view?set_language=en Bernstein Network for Computational Neuroscience].
* [http://www.areadne.org/index.html AREADNE Conferences]– a biennial meeting that includes theoretical and experimental results, held in even years in Santorini, Greece.

===Websites===
* [http://home.earthlink.net/~perlewitz/ Perlewitz's computational neuroscience on the web]
* [http://www.scholarpedia.org/article/Encyclopedia_of_Computational_Neuroscience Encyclopedia of Computational Neuroscience], part of [[Scholarpedia]], an online expert curated encyclopedia on computational neuroscience, dynamical systems and machine intelligence

{{Neuroscience}}
{{Cybernetics}}

{{DEFAULTSORT:Computational Neuroscience}}
[[Category:Computational neuroscience|*]]
[[Category:Cybernetics]]
[[Category:Cognitive neuroscience]]

[[de:Computational Neuroscience]]
[[es:Neurociencia computacional]]
[[fa:عصب‌شناسی محاسباتی]]
[[fr:Neurosciences computationnelles]]
[[ja:計算論的神経科学]]
[[pt:Neurociência computacional]]
[[zh:計算神經科學]]